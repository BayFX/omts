# Expert Panel Review: Graph Data Modeling Perspective

**Reviewer:** Graph Modeling Expert, Graph Data Modeling & Algorithm Specialist
**Scope:** Full spec suite (SPEC-001 through SPEC-006)
**Date:** 2026-02-18

---

## Assessment

The OMTSF specification suite makes a well-considered choice in adopting the directed labeled property multigraph as its foundational data model. This aligns directly with the property graph model formalized in ISO/IEC 39075 (GQL), published April 2024, and is the dominant model in modern graph databases (Neo4j, TigerGraph, Amazon Neptune). The decision to use a flat adjacency list serialization -- separate `nodes[]` and `edges[]` arrays with typed, attributed elements and independent edge IDs -- is structurally sound. It enables O(|V| + |E|) in-memory graph construction, supports multigraph semantics natively (multiple edges of the same type between the same node pair, distinguished by edge ID), and avoids the deep nesting problems of tree-serialized graph formats. The model is expressive enough to represent the supply chain domain's core structures: corporate hierarchies (DAGs with permitted cross-holdings), supply networks (cyclic directed multigraphs), and attestation linkages (bipartite-like subgraphs connecting entities to certifications).

The merge semantics in SPEC-003 are the most technically ambitious component and are largely well-designed from a graph-theoretic standpoint. The identity predicate based on shared external identifiers, combined with union-find for transitive closure computation, is the correct algorithmic approach for entity resolution at scale. The stated algebraic properties -- commutativity, associativity, idempotency -- are exactly the properties required for a CRDT-like decentralized merge model, and they follow from the symmetric identity predicate and union-based property merging. The edge merge-identity table (Section 3.1) is a pragmatic solution to the multigraph disambiguation problem: in the absence of explicit edge identifiers, a composite key of resolved endpoints plus type-specific discriminant properties determines identity.

However, the spec suite has gaps that will impede graph-native tooling integration and may introduce correctness risks at scale. The cycle constraint model is underspecified, the graph lacks explicit support for hyperedges or n-ary relationships (important for multi-party supply arrangements), and the merge procedure does not address the false-positive transitive closure problem that arises when identifier-based entity resolution produces erroneous bridges between unrelated equivalence classes. These issues are tractable but need attention before implementations encounter them in production graphs.

---

## Strengths

- **Correct graph model selection.** The directed labeled property multigraph is the right abstraction. It matches the ISO/IEC 39075 (GQL) property graph model, enabling future query interoperability with any GQL-compliant graph database. The explicit edge IDs support multigraph semantics cleanly.
- **Flat adjacency list serialization.** The `nodes[]` / `edges[]` structure enables single-pass graph construction. An implementation reads nodes into a hash map (O(|V|)), then wires edges by source/target lookup (O(|E|)), yielding an in-memory adjacency list in linear time. This is well-suited for the Property Graph Exchange Format (PG) interop path.
- **Union-find for merge.** SPEC-003 correctly identifies union-find (disjoint set forest) as the appropriate algorithm for computing transitive closure of merge candidates. The O(n * alpha(n)) amortized complexity is effectively linear and scales to million-node graphs without concern.
- **Nuanced cycle policy.** Permitting cycles in supply subgraphs (recycling loops are real) while enforcing acyclicity in `legal_parentage` (forest constraint, L3-MRG-02) reflects genuine domain modeling. Cross-holdings in `ownership` are also correctly permitted with circular 100% as an L2 warning.
- **Typed edge constraints.** Each edge type specifies legal source/target node type pairs (e.g., `beneficial_ownership` requires `person` -> `organization`). This amounts to a graph schema -- typed edges with domain/range restrictions -- which is essential for validation and query optimization.
- **Temporal validity on identifiers and edges.** The `valid_from` / `valid_to` pattern on both identifiers (SPEC-002) and edges (SPEC-001) enables bitemporal graph analysis. The temporal compatibility check in the merge identity predicate (SPEC-003, Section 2, condition 4) prevents false merges from reassigned identifiers -- a real problem with DUNS and GLN.
- **`same_as` edge with confidence levels.** This is a principled approach to uncertain entity resolution. Modeling uncertain equivalence as advisory edges with `definite`/`probable`/`possible` confidence, rather than forcing premature merge, preserves information and supports human-in-the-loop resolution.
- **Edge property wrapper (`properties` object).** As the Data Format Expert notes, the wrapper approach aligns with labeled property graph tooling conventions. Neo4j's import formats and the PG exchange format both expect properties as a distinct map, not mixed with structural fields. SPEC-001 Section 2.1 now explicitly documents this.

---

## Concerns

- **[Major] Transitive closure false-positive amplification.** The merge procedure (SPEC-003, Section 4, step 3) computes full transitive closure: if X matches Y and Y matches Z, all three merge. This is correct in theory but dangerous in practice. A single erroneous identifier match (e.g., a mistyped DUNS number) can bridge two large, unrelated equivalence classes, cascading into massive incorrect merges. The spec acknowledges the enrichment-merge interaction (Section 9) but provides no mechanism for detecting or limiting false-positive propagation -- no merge-candidate confidence scoring, no maximum merge-group size heuristic, no "split" operation for correcting bad merges. Real-world entity resolution systems (e.g., Senzing, Zingg) always include a feedback loop for this; OMTSF's pure transitive closure model lacks one.

- **[Major] No explicit graph schema definition language.** The node type and edge type constraints are defined in prose (e.g., `ownership` goes from `organization` to `organization`; `beneficial_ownership` from `person` to `organization`). There is no formal graph schema definition -- no equivalent of Cypher's `CREATE CONSTRAINT`, GQL's graph type definitions, or a schema language like SHACL/ShEx for RDF. Without a machine-readable graph schema, validators must hand-code every type constraint, and graph database imports cannot auto-generate schema constraints. This is distinct from the JSON Schema concern (which covers serialization structure); this is about the graph topology constraints.

- **[Major] No support for n-ary relationships or hyperedges.** Some supply chain relationships are inherently multi-party: a tolling arrangement may involve a material owner, a toll processor, and a logistics provider simultaneously. The current model forces decomposition into multiple binary edges, losing the atomicity of the relationship. Similarly, a `composed_of` BOM relationship with quantity and unit attributes is really a weighted hyperedge connecting a parent good to multiple components. The reification pattern (modeling the n-ary relationship as a node with edges to participants) is the standard workaround in property graphs, but the spec does not document this pattern or provide guidance.

- **[Minor] Cycle detection requirements are underspecified.** SPEC-003 L3-MRG-02 requires `legal_parentage` to form a forest, but the validation level is L3 (enrichment tooling), not L1 or L2. This means a structurally valid file can contain cycles in `legal_parentage`. For graph algorithms that assume DAG structure on this subgraph (e.g., topological sort for corporate hierarchy traversal), encountering an unexpected cycle could cause infinite loops or incorrect results. The forest constraint should be at least L2, or the spec should explicitly state that consumers must handle cycles defensively.

- **[Minor] No graph-level metadata for algorithm hints.** The file header has `omtsf_version`, `snapshot_date`, `file_salt`, and `disclosure_scope`, but no fields for graph-level statistics (node count, edge count, density, connected component count) or structural hints (is the graph acyclic? is it bipartite in any subgraph?). These metadata fields would enable consumers to select appropriate algorithms and allocate resources before parsing the full graph. The advisory size limits in Section 9.4 are a step in the right direction but are producer-side recommendations, not consumer-facing metadata.

- **[Minor] `boundary_ref` nodes create phantom connectivity.** When a node is redacted and replaced with a `boundary_ref`, all edges incident on the original node are rewired to the boundary reference. If two redacted nodes are replaced by boundary references with different opaque IDs, the graph topology is preserved correctly. But if the same entity appears in two different redacted regions and gets two different boundary references (fresh salt per file, by design), the graph loses a potential merge candidate. SPEC-004 acknowledges this tradeoff explicitly ("privacy over temporal tracking"), which is appropriate, but the graph-theoretic consequence is that post-redaction graphs may have artificially inflated connected component counts and underestimate true network density.

---

## Recommendations

1. **[P0] Add merge-group size limits and confidence scoring.** Define an advisory maximum merge-group size (e.g., 50 nodes). When transitive closure produces a group exceeding this threshold, implementations SHOULD emit a warning and MAY split the group into sub-groups requiring human review. Additionally, define a merge-confidence metric based on the number and type of shared identifiers (e.g., LEI match = high confidence; single DUNS match = medium; name-only `same_as` = low). This provides a safety net against false-positive cascading.

2. **[P1] Publish a formal graph schema.** Define a machine-readable graph type schema specifying, for each edge type, the permitted source and target node types, required and optional properties, and cardinality constraints (e.g., `operates`: source must be `organization`, target must be `facility`, zero or more per pair). This could be a JSON document in the repository (`schema/omts-graph-type.json`) using a simple custom format or aligned with the GQL graph type definition syntax. This schema would drive import into Neo4j, Neptune, and TigerGraph, and would enable generic graph validation tooling.

3. **[P1] Document the reification pattern for n-ary relationships.** Add an informative section (or an appendix to SPEC-001) showing how multi-party supply relationships are modeled using intermediate nodes. For example, a three-party tolling arrangement can be modeled as an intermediate `arrangement` node (custom type via extension mechanism) with edges to each participant. This guidance prevents inconsistent ad-hoc solutions across implementations.

4. **[P1] Promote the `legal_parentage` forest constraint to L2.** Cycles in `legal_parentage` are always data errors (a company cannot be its own indirect parent). Detecting cycles in a directed subgraph is O(|V| + |E|) via DFS-based topological sort, which any L2 validator can perform without external data. Keeping it at L3 risks implementations silently producing graphs with broken corporate hierarchy semantics.

5. **[P2] Add graph-level summary metadata.** Extend the file header with an optional `graph_summary` object: `{ "node_count": 1234, "edge_count": 5678, "node_type_counts": { "organization": 500, ... }, "edge_type_counts": { "supplies": 2000, ... } }`. This allows consumers to pre-allocate data structures and display progress during loading. The counts are advisory (validators need not verify them) but improve the developer experience for graph-native tooling.

6. **[P2] Define graph export guidance for major graph databases.** Add an informative appendix (or extend SPEC-006) showing how `.omts` files map to Neo4j Cypher `LOAD CSV` / `apoc.load.json`, Amazon Neptune Gremlin bulk loader format, and TigerGraph GSQL loading jobs. This reduces friction for graph-native consumers and validates that the data model round-trips through real graph databases without information loss.

---

## Cross-Expert Notes

- **To Systems Engineering Expert:** The union-find implementation for merge is straightforward in Rust (a `Vec<usize>` with path compression and union by rank). However, the false-positive transitive closure problem (see Concern 1) means the implementation should expose merge-group introspection -- e.g., returning the full list of merge groups with their constituent nodes and shared identifiers -- so that downstream tooling can audit and override merge decisions. Do not treat merge as a black-box operation.

- **To Data Format Expert:** Your concern about the edge `properties` wrapper is directly relevant to graph database interoperability. The wrapper approach (`"properties": { ... }`) is the correct choice for labeled property graph tooling (Neo4j, TigerGraph). Flat properties on the edge object would require transformation on import. I concur that SPEC-001 Section 2.1's explicit normative statement resolving this ambiguity is essential.

- **To Entity Identification Expert:** The temporal compatibility check on identifiers (SPEC-003, Section 2, condition 4) is critical for preventing false merges from reassigned identifiers. However, it depends on producers actually populating `valid_from`/`valid_to` on identifiers -- which is only an L2 recommendation, not an L1 requirement. For schemes known to reassign values (DUNS, GLN), consider promoting temporal fields to L1 for those specific schemes, or at least making the merge engine emit a warning when matching on temporally-unqualified DUNS/GLN values.

- **To Security & Privacy Expert:** The `boundary_ref` hashing approach (SPEC-004) is sound from a graph-theoretic standpoint -- it preserves topological connectivity while hiding node identity. The fresh-salt-per-file design prevents cross-file correlation of redacted nodes, which is the right privacy tradeoff. However, an adversary with access to the un-redacted graph and the redacted graph can potentially reconstruct boundary reference assignments via graph isomorphism attacks (matching neighborhood structure around boundary refs). The spec should acknowledge this residual risk in a security considerations section.

- **To Standards Expert:** The alignment with ISO/IEC 39075 (GQL) property graph model is a strategic advantage. As GQL adoption grows in graph databases, OMTSF files should be queryable via GQL with minimal transformation. I recommend explicitly citing ISO/IEC 39075:2024 in SPEC-006 (Standards Mapping) and documenting the mapping between OMTSF node/edge types and GQL graph type definitions. The Property Graph Exchange Format (PG) specification at pg-format.github.io is also worth referencing as a potential interoperability target.

- **To Supply Chain Expert:** The `composed_of` edge type for BOM decomposition is a good start, but real manufacturing BOMs are often multi-level DAGs with quantity multipliers at each level. Traversing a `composed_of` subgraph to compute total material requirements is a standard graph reachability problem with path-weight accumulation. The spec should note that `composed_of` subgraphs are expected to be DAGs (no circular BOMs) and that tooling can compute aggregate quantities via depth-first traversal with multiplicative weight propagation.
